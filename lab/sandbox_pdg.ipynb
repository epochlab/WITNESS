{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "from arch import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = utils.device_mapper()\n",
    "print(f\"Device: {str(DEVICE).upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", variant='fp16', torch_dtype=torch.float16)\n",
    "pipeline = pipeline.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/puppy.png')\n",
    "init_image = utils.load_image(PATH)\n",
    "init_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    w, h = map(lambda x: x - x % 32, img.size)  # Resize to integer multiple of 32\n",
    "    img = img.resize((w, h), resample=Image.LANCZOS)\n",
    "    img = utils.image2array(img)\n",
    "    img = img[None].transpose(0, 3, 1, 2)\n",
    "    img = torch.from_numpy(img)\n",
    "    return 2.0 * img - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(x, model, eps=0.1, step_size=0.015, iters=40):\n",
    "    x_adv = x.clone().detach() + (torch.rand(*x.shape) * 2 * eps-eps).to(DEVICE)\n",
    "\n",
    "    for i in (t := trange(iters)):\n",
    "        actual_step_size = step_size - (step_size - step_size / 100) / iters * i  \n",
    "\n",
    "        x_adv.requires_grad_(True)\n",
    "        loss = (model(x_adv).latent_dist.mean).norm()\n",
    "\n",
    "        t.set_description(f\"[Running attack]: Loss {loss.item():.5f} | step size: {actual_step_size:.4}\")\n",
    "\n",
    "        grad, = torch.autograd.grad(loss, [x_adv])\n",
    "        x_adv = x_adv - grad.detach().sign() * actual_step_size\n",
    "        x_adv = torch.minimum(torch.maximum(x_adv, x - eps), x + eps)\n",
    "        x_adv.data = torch.clamp(x_adv, min=-1, max=1)\n",
    "        x_adv.grad = None\n",
    "\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps | Higher = Less imperceptible\n",
    "# step | Set smaller than eps\n",
    "# iters | Higher = Stronger\n",
    "\n",
    "with torch.autocast(str(DEVICE)):\n",
    "    X = preprocess(init_image).half().to(DEVICE)\n",
    "    adv_X = pgd_attack(X, model=pipeline.vae.encode, eps=0.06, step_size=0.02, iters=1000)\n",
    "    adv_X = (adv_X / 2 + 0.5).clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPIL = T.ToPILImage()\n",
    "\n",
    "adv_image = toPIL(adv_X[0]).convert(\"RGB\")\n",
    "adv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperConfig:\n",
    "    prompt = \"dog under heavy rain and muddy ground real\"\n",
    "    strength = 0.5\n",
    "    infer_steps = 50\n",
    "    cfg_scale = 7.0\n",
    "    seed = 9222\n",
    "\n",
    "config = HyperConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast('cuda'):\n",
    "    image_nat = pipeline(prompt=config.prompt, \n",
    "                         image=init_image, \n",
    "                         strength=config.strength, \n",
    "                         guidance_scale=config.cfg_scale, \n",
    "                         num_inference_steps=config.infer_steps).images[0]\n",
    "    \n",
    "    image_adv = pipeline(prompt=config.prompt, \n",
    "                         image=adv_image, \n",
    "                         strength=config.strength, \n",
    "                         guidance_scale=config.cfg_scale, \n",
    "                         num_inference_steps=config.infer_steps).images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20,6))\n",
    "\n",
    "ax[0].imshow(init_image)\n",
    "ax[1].imshow(adv_image)\n",
    "ax[2].imshow(image_nat)\n",
    "ax[3].imshow(image_adv)\n",
    "\n",
    "ax[0].set_title('Source Image', fontsize=12)\n",
    "ax[1].set_title('Adv Image', fontsize=12)\n",
    "ax[2].set_title('Gen. Image Nat.', fontsize=12)\n",
    "ax[3].set_title('Gen. Image Adv.', fontsize=12)\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].grid(False), ax[i].axis('off')\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
